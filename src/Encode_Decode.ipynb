{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from data_scripts.LevelDataset import LevelDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from converter.to_img_converter.MultiLayerStackDecoder import MultiLayerStackDecoder\n",
    "from level.LevelVisualizer import LevelVisualizer\n",
    "from level.LevelReader import LevelReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"train_datasets/test_run_200/test_run_200.tfrecords\"\n",
    "dataset = LevelDataset(dataset_path = dataset_path, batch_size = 1)\n",
    "dataset.load_dataset()\n",
    "iter_data = dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 5)\n",
      "dict_keys(['block_amount', 'level_height', 'level_width', 'pig_amount', 'pixel_height', 'pixel_width', 'platform_amount', 'special_block_amount'])\n"
     ]
    }
   ],
   "source": [
    "for image_batch, data in iter_data:\n",
    "\n",
    "    # print(data)\n",
    "    print(image_batch.shape)\n",
    "    print(data.keys())\n",
    "    original = image_batch[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_level_decoder():\n",
    "    multilayer_stack_decoder = MultiLayerStackDecoder()\n",
    "    multilayer_stack_decoder.round_to_next_int = True\n",
    "    multilayer_stack_decoder.custom_kernel_scale = True\n",
    "    multilayer_stack_decoder.minus_one_border = False\n",
    "    multilayer_stack_decoder.combine_layers = True\n",
    "    multilayer_stack_decoder.negative_air_value = -1\n",
    "    multilayer_stack_decoder.cutoff_point = 0.5\n",
    "    multilayer_stack_decoder.display_decoding = False\n",
    "    return multilayer_stack_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom environment\n",
    "class Environment:\n",
    "    def init(self, map, max_step=math.inf):\n",
    "        self.map = map\n",
    "        self.state = copy.deepcopy(map)\n",
    "        self.step_count = 0\n",
    "        self.max_step = max_step\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.deepcopy(self.map)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "\n",
    "        state_tensor = torch.tensor(self.state)\n",
    "        return state_tensor\n",
    "\n",
    "    def step(self, action, distance):\n",
    "        # add that pixel to the state (level map)\n",
    "        # action[0] is the x coordinate, action[1] is the y coordinate, action[2] is the type\n",
    "        self.state[action[0], action[1], action[2]] = 1\n",
    "        self.step_count += 1\n",
    "        # reward = self.reward_func(action[0], action[1], action[2], distance)\n",
    "        reward = 0\n",
    "\n",
    "        if self.step_count > self.max_step:\n",
    "            self.done = True\n",
    "\n",
    "        info = {'step': self.step_count, 'action': action, 'reward': reward, 'done': self.done}\n",
    "\n",
    "        return torch.tensor(self.state), torch.tensor(reward), self.done, info\n",
    "    \n",
    "    def reward_func(action, average_pixel):\n",
    "        # get the distance from average of the empty pixels\n",
    "        return 1/(1+math.sqrt((action[0] - average_pixel[0])**2 + (action[1] - average_pixel[1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"train_datasets/modified_test_run_200/modified_test_run_200.tfrecords\"\n",
    "modified_dataset = LevelDataset(dataset_path = dataset_path, batch_size = 1)\n",
    "modified_dataset.load_dataset()\n",
    "modified_iter_data = modified_dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 5)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, data in modified_iter_data:\n",
    "    # print(data)\n",
    "    print(image_batch.shape)\n",
    "    modified = image_batch[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python_VE\\ai_games\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from evaluation.GridSearchDecode import run_evaluation_xml_levels_one_by_one, create_tests\n",
    "\n",
    "parameters = create_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def move_file(source_file, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    if os.path.exists(source_file):\n",
    "        file_name = source_file.split(\"/\")[-1]\n",
    "        destination_file = os.path.join(destination_folder, file_name)\n",
    "        shutil.move(source_file, destination_file)\n",
    "        print(f\"File '{source_file}' moved successfully!\")\n",
    "        return destination_file\n",
    "    else:\n",
    "        print(f\"File '{source_file}' does not exist in the source folder.\")\n",
    "\n",
    "# Example usage:\n",
    "# move_file(\"path/to/source/folder\", \"path/to/destination/folder\", \"filename.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_distance(predicted_pixel, pixel_list):\n",
    "    min_distance = math.inf\n",
    "    for pixel in pixel_list:\n",
    "        distance = math.sqrt((predicted_pixel[0] - pixel[0])**2 + (predicted_pixel[1] - pixel[1])**2 + (predicted_pixel[2] - pixel[2])**2)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "\n",
    "    reward = 1/(1+min_distance)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_difference(original, modified, predicted_pixel):\n",
    "    mask = tf.not_equal(original, modified)\n",
    "    indices = tf.where(mask)\n",
    "\n",
    "    # Print the positions where the elements are different\n",
    "    # print(\"Positions where elements are different:\")\n",
    "    # print(np.where(mask))\n",
    "\n",
    "    num_differences = np.sum(mask)\n",
    "\n",
    "    # Print the number of differences\n",
    "    # print(\"Number of differences:\", num_differences)\n",
    "\n",
    "    pixel_list = []\n",
    "\n",
    "    for idx in indices:\n",
    "        # row_idx, col_idx, channel_idx = idx[0], idx[1], idx[2]\n",
    "        # original_value = tf.gather_nd(original, [idx])\n",
    "        # modified_value = tf.gather_nd(modified, [idx])\n",
    "        pixel_list.append(idx)\n",
    "        # print(f\"Difference at position {idx}: original={original_value.numpy()}, modified={modified_value.numpy()}\")\n",
    "\n",
    "    return find_closest_distance(predicted_pixel, pixel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from converter.gan_processing.DecodingFunctions import DecodingFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_convert(original, modified, path, show_fig = False):\n",
    "    # functions to move the output from [-1, 1] to [0, 1] range\n",
    "    decoding_functions = DecodingFunctions(threshold_callback = lambda: 0.5)\n",
    "    decoding_functions.set_rescaling(rescaling = tf.keras.layers.Rescaling)\n",
    "    decoding_functions.update_rescale_values(max_value = 1, shift_value = 1)\n",
    "    rescale_function = decoding_functions.rescale\n",
    "\n",
    "    # function to flatten the gan output to an image with 1 channel\n",
    "    decoding_function = decoding_functions.argmax_multilayer_decoding_with_air\n",
    "\n",
    "    if show_fig:\n",
    "        ref_img, _ = decoding_function(original)\n",
    "        # print(gan_outputs_reformatted[i].shape)\n",
    "\n",
    "        # save image trough matplotlib\n",
    "        plt.imshow(ref_img)\n",
    "        plt.savefig(f'{path}/original_level.png')\n",
    "        # clear plot\n",
    "        plt.clf()\n",
    "\n",
    "        ref_img, _ = decoding_function(modified)\n",
    "        # print(gan_outputs_reformatted[i].shape)\n",
    "\n",
    "        # save image trough matplotlib\n",
    "        plt.imshow(ref_img)\n",
    "        plt.savefig(f'{path}/modified1_level.png')\n",
    "        # clear plot\n",
    "        plt.clf()\n",
    "\n",
    "    multilayer_stack_decoder = load_level_decoder()\n",
    "    # level_visualizer = LevelVisualizer()\n",
    "    level_reader = LevelReader()\n",
    "\n",
    "    \n",
    "    level = multilayer_stack_decoder.decode(modified)\n",
    "    # print(\"level\", level)\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, dpi = 100)\n",
    "    # level_visualizer.create_img_of_structure(\n",
    "    #     level.get_used_elements(), use_grid = False, ax = ax, scaled = True\n",
    "    # )\n",
    "    # fig.savefig('modified1_decoded_level.png')\n",
    "    # plt.clf()\n",
    "\n",
    "    # Save level to xml\n",
    "    level_xml = level_reader.create_level_from_structure(level.get_used_elements(), red_birds = True, move_to_ground = True)\n",
    "    level_reader.write_xml_file(level_xml, os.path.join(\"./\", f'{path}/modified_level.xml'))\n",
    "\n",
    "    return f'{path}/modified_level.xml'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of differences: 145\n",
      "Number of differences: 146\n",
      "Number of differences: 147\n",
      "Number of differences: 148\n",
      "Number of differences: 148\n",
      "Number of differences: 148\n",
      "Number of differences: 149\n",
      "Number of differences: 150\n",
      "Number of differences: 151\n",
      "Number of differences: 152\n",
      "Number of differences: 153\n",
      "Number of differences: 154\n",
      "Number of differences: 154\n",
      "Number of differences: 155\n",
      "Number of differences: 156\n",
      "Number of differences: 157\n",
      "Number of differences: 158\n",
      "Number of differences: 159\n",
      "Number of differences: 159\n",
      "Number of differences: 160\n",
      "Number of differences: 161\n",
      "Number of differences: 161\n",
      "Number of differences: 162\n",
      "Number of differences: 162\n",
      "Number of differences: 163\n",
      "Number of differences: 164\n",
      "Number of differences: 165\n",
      "Number of differences: 166\n",
      "Number of differences: 167\n",
      "Number of differences: 168\n",
      "Number of differences: 169\n",
      "Number of differences: 170\n",
      "Number of differences: 171\n",
      "Number of differences: 171\n",
      "Number of differences: 172\n",
      "Number of differences: 173\n",
      "Number of differences: 174\n",
      "Number of differences: 175\n",
      "Number of differences: 176\n",
      "Number of differences: 177\n",
      "Number of differences: 177\n",
      "Number of differences: 178\n",
      "Number of differences: 178\n",
      "Number of differences: 179\n",
      "Number of differences: 180\n",
      "Number of differences: 181\n",
      "Number of differences: 181\n",
      "Number of differences: 181\n",
      "Number of differences: 182\n",
      "Number of differences: 183\n",
      "Number of differences: 184\n",
      "Number of differences: 185\n",
      "Number of differences: 186\n",
      "Number of differences: 187\n",
      "Number of differences: 188\n",
      "Number of differences: 189\n",
      "Number of differences: 190\n",
      "Number of differences: 191\n",
      "Number of differences: 192\n",
      "Number of differences: 193\n",
      "Number of differences: 194\n",
      "Number of differences: 194\n",
      "Number of differences: 195\n",
      "Number of differences: 196\n",
      "Number of differences: 197\n",
      "Number of differences: 198\n",
      "Number of differences: 199\n",
      "Number of differences: 200\n",
      "Number of differences: 201\n",
      "Number of differences: 202\n",
      "Number of differences: 203\n",
      "Number of differences: 204\n",
      "Number of differences: 205\n",
      "Number of differences: 205\n",
      "Number of differences: 206\n",
      "Number of differences: 207\n",
      "Number of differences: 208\n",
      "Number of differences: 209\n",
      "Number of differences: 210\n",
      "Number of differences: 211\n",
      "Number of differences: 212\n",
      "Number of differences: 213\n",
      "Number of differences: 214\n",
      "Number of differences: 214\n",
      "Number of differences: 215\n",
      "Number of differences: 216\n",
      "Number of differences: 217\n",
      "Number of differences: 218\n",
      "Number of differences: 219\n",
      "Number of differences: 220\n",
      "Number of differences: 220\n",
      "Number of differences: 221\n",
      "Number of differences: 221\n",
      "Number of differences: 222\n",
      "Number of differences: 223\n",
      "Number of differences: 224\n",
      "Number of differences: 224\n",
      "Number of differences: 225\n",
      "Number of differences: 226\n",
      "Number of differences: 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farro\\AppData\\Local\\Temp\\ipykernel_4552\\2063465691.py:87: UserWarning: Using a target size (torch.Size([100, 100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = -torch.min(surr1, surr2) + torch.nn.functional.smooth_l1_loss(self.v(s), td_target.detach()) - 0.01 * entropy\n",
      "2024-04-17 20:10:33.867 | DEBUG    | game_management.GameManager:start_game:39 - Start Game and Game Connection Server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level Path: created, Blocks: 17 Pigs: 1 Platform: 0 Bird: 0\n",
      "File 'temp/modified_level.xml' moved successfully!\n",
      "Start game: resources\\science_birds\\win-new\\ScienceBirds.exe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 20:10:38.021 | DEBUG    | evaluation.GridSearchDecode:run_evaluation_xml_levels_one_by_one:288 - Run parameters: {'round_to_next_int': True, 'custom_kernel_scale': True, 'minus_one_border': True, 'combine_layers': True, 'negative_air_value': -10, 'cutoff_point': 0.1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Run time: 0.0010075569152832031\n",
      "Number of stable levels:  0\n",
      "List of stable level names: []\n",
      "Level is stable\n",
      "Episode Score: 757.5856038267266, Loss: -115.95477294921875\n",
      "Number of differences: 145\n",
      "Number of differences: 145\n",
      "Number of differences: 146\n",
      "Number of differences: 147\n",
      "Number of differences: 148\n",
      "Number of differences: 149\n",
      "Number of differences: 150\n",
      "Number of differences: 150\n",
      "Number of differences: 151\n",
      "Number of differences: 151\n",
      "Number of differences: 152\n",
      "Number of differences: 153\n",
      "Number of differences: 154\n",
      "Number of differences: 155\n",
      "Number of differences: 156\n",
      "Number of differences: 157\n",
      "Number of differences: 157\n",
      "Number of differences: 157\n",
      "Number of differences: 157\n",
      "Number of differences: 158\n",
      "Number of differences: 158\n",
      "Number of differences: 159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 139\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# Assume modified_iter_data is your dataset\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified_iter_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 121\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(iter_data, modified_iter_data)\u001b[0m\n\u001b[0;32m    119\u001b[0m     state \u001b[38;5;241m=\u001b[39m state_prime\n\u001b[0;32m    120\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Update score\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_net()  \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    124\u001b[0m xml_path \u001b[38;5;241m=\u001b[39m xml_convert(original, env\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, show_fig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m, in \u001b[0;36mcalculate_difference\u001b[1;34m(original, modified, predicted_pixel)\u001b[0m\n\u001b[0;32m     22\u001b[0m     pixel_list\u001b[38;5;241m.\u001b[39mappend(idx)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# print(f\"Difference at position {idx}: original={original_value.numpy()}, modified={modified_value.numpy()}\")\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_closest_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_pixel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mfind_closest_distance\u001b[1;34m(predicted_pixel, pixel_list)\u001b[0m\n\u001b[0;32m      2\u001b[0m min_distance \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pixel \u001b[38;5;129;01min\u001b[39;00m pixel_list:\n\u001b[1;32m----> 4\u001b[0m     distance \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt((predicted_pixel[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43mpixel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (predicted_pixel[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m pixel[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (predicted_pixel[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m pixel[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance \u001b[38;5;241m<\u001b[39m min_distance:\n\u001b[0;32m      6\u001b[0m         min_distance \u001b[38;5;241m=\u001b[39m distance\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1052\u001b[0m, in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1050\u001b[0m   var_empty \u001b[38;5;241m=\u001b[39m constant([], dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m   1051\u001b[0m   packed_begin \u001b[38;5;241m=\u001b[39m packed_end \u001b[38;5;241m=\u001b[39m packed_strides \u001b[38;5;241m=\u001b[39m var_empty\n\u001b[1;32m-> 1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstrided_slice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_begin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_strides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1225\u001b[0m, in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m   strides \u001b[38;5;241m=\u001b[39m ones_like(begin)\n\u001b[1;32m-> 1225\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrided_slice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1237\u001b[0m parent_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Python_VE\\ai_games\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:10664\u001b[0m, in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  10663\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10664\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10665\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStridedSlice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbegin_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10666\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mellipsis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10667\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_axis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshrink_axis_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  10669\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Define a custom environment\n",
    "class Environment:\n",
    "    def __init__(self, map, max_step=100):\n",
    "        self.map = map  # Initialize the map\n",
    "        self.state = copy.deepcopy(self.map)\n",
    "        self.step_count = 0\n",
    "        self.max_step = max_step\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.deepcopy(self.map)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        state_tensor = torch.tensor(self.state, dtype=torch.float32).flatten()\n",
    "        return state_tensor\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y, z = action  # Decompose the action into coordinates\n",
    "        if self.state[x, y, z] == -1:\n",
    "            self.state[x, y, z] = 1  # Place block if the spot is empty\n",
    "            reward = 10  # Positive reward for placing a block\n",
    "        else:\n",
    "            reward = -10 # Negative reward if block is already there\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.max_step:\n",
    "            self.done = True\n",
    "        return torch.tensor(self.state, dtype=torch.float32).flatten(), reward, self.done, {}\n",
    "\n",
    "# PPO Model\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_pi = nn.Linear(128, input_size)\n",
    "        self.fc_v = nn.Linear(128, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0003)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.99)\n",
    "        self.data = []  # Initialize data list for storing transitions\n",
    "\n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = torch.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = zip(*self.data)\n",
    "        self.data = []\n",
    "        return torch.stack(s_lst), torch.tensor(a_lst, dtype=torch.long), torch.tensor(r_lst, dtype=torch.float), torch.stack(s_prime_lst), torch.tensor(done_lst, dtype=torch.float)\n",
    "\n",
    "    def train_net(self, gamma=0.98, lmbda=0.95, eps_clip=0.2):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "        td_target = r + gamma * self.v(s_prime) * (1 - done)\n",
    "        delta = td_target - self.v(s)\n",
    "        delta = delta.detach().numpy()\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        ratio = torch.exp(torch.log(pi_a) - torch.log(pi_a.detach()))\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "        entropy = -(pi * torch.log(pi + 1e-5)).sum(1).mean()\n",
    "        loss = -torch.min(surr1, surr2) + torch.nn.functional.smooth_l1_loss(self.v(s), td_target.detach()) - 0.01 * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_mean = loss.mean()  # Compute mean loss\n",
    "        loss_mean.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        return loss_mean  # Return the mean loss value\n",
    "\n",
    "\n",
    "\n",
    "def main(iter_data, modified_iter_data):\n",
    "    model = PPO(128*128*5)  # Initialize the PPO model\n",
    "    score = 0.\n",
    "    counter = 0\n",
    "    combined_dataset = tf.data.Dataset.zip((iter_data, modified_iter_data))\n",
    "\n",
    "    for (image_batch, data), (modified_image_batch, modified_data) in combined_dataset:\n",
    "        \n",
    "        original = image_batch[0].numpy()\n",
    "        score = 0.0\n",
    "        env = Environment(modified_image_batch[0].numpy())  # Create environment from batch\n",
    "        state = env.reset()  # Reset environment at the start of each batch\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            prob = model.pi(state)  # Policy forward pass\n",
    "            m = Categorical(prob)  # Distribution for sampling actions\n",
    "            action_index = m.sample().item()  # Sample an action\n",
    "            action = (action_index // (128*128), (action_index % (128*128)) // 128, action_index % 5)  # Decode action index to 3D action\n",
    "            state_prime, reward, done, _ = env.step(action)  # Execute action in the environment\n",
    "        \n",
    "            model.put_data((state, action_index, reward, state_prime, done))  # Store data for training\n",
    "            state = state_prime\n",
    "            score += reward  # Update score\n",
    "            score += calculate_difference(original, env.state, action)\n",
    "        loss = model.train_net()  # Train model\n",
    "\n",
    "        xml_path = xml_convert(original, env.state, \"temp\", show_fig = False)\n",
    "        final_path = move_file(xml_path, 'evaluation\\\\temp')\n",
    "        # if level is stable\n",
    "        if run_evaluation_xml_levels_one_by_one(\"temp\", parameters[0]):\n",
    "            print(\"Level is stable\")\n",
    "            score += 10\n",
    "        else:\n",
    "            print(\"Level is not stable\")\n",
    "            score -= 10\n",
    "        os.remove(final_path)\n",
    "\n",
    "        print(f\"Episode Score: {score}, Loss: {loss}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Assume modified_iter_data is your dataset\n",
    "    main(iter_data, modified_iter_data)\n",
    "    # Save the model\n",
    "    print(\"Model saved successfully\")\n",
    "    torch.save(model.state_dict(), 'ppo_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "print(\"Loading ...\")\n",
    "model = PPO(128*128*5)  # Recreate the model\n",
    "model.load_state_dict(torch.load('ppo_model.pth'))  # Load the saved parameters\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for image_batch, data in modified_iter_data:  # Assuming test data is provided in test_iter_data\n",
    "        state = env.reset()  # Reset the environment\n",
    "        done = False\n",
    "        while not done:\n",
    "            prob = model.pi(state)\n",
    "            m = Categorical(prob)\n",
    "            action_index = m.sample().item()\n",
    "            action = (action_index // (128*128), (action_index % (128*128)) // 128, action_index % 5)  \n",
    "            state_prime, reward, done, _ = env.step(action)\n",
    "            state = state_prime\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for idx in indices:\n",
    "#     count += 1\n",
    "\n",
    "#     row_idx, col_idx, channel_idx = idx[0], idx[1], idx[2]\n",
    "#     original_value = tf.gather_nd(original, [idx])  # Get original value\n",
    "#     modified = tf.tensor_scatter_nd_update(\n",
    "#         modified, [[row_idx, col_idx, channel_idx]], original_value  # Update modified tensor\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Modified tensor with original values rewritten:\")\n",
    "# print(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
