{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from data_scripts.LevelDataset import LevelDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from converter.to_img_converter.MultiLayerStackDecoder import MultiLayerStackDecoder\n",
    "from level.LevelVisualizer import LevelVisualizer\n",
    "from level.LevelReader import LevelReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"train_datasets/test_run_200/test_run_200.tfrecords\"\n",
    "dataset = LevelDataset(dataset_path = dataset_path, batch_size = 1)\n",
    "dataset.load_dataset()\n",
    "iter_data = dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, data in iter_data:\n",
    "\n",
    "    # print(data)\n",
    "    print(image_batch.shape)\n",
    "    print(data.keys())\n",
    "    original = image_batch[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_level_decoder():\n",
    "    multilayer_stack_decoder = MultiLayerStackDecoder()\n",
    "    multilayer_stack_decoder.round_to_next_int = True\n",
    "    multilayer_stack_decoder.custom_kernel_scale = True\n",
    "    multilayer_stack_decoder.minus_one_border = False\n",
    "    multilayer_stack_decoder.combine_layers = True\n",
    "    multilayer_stack_decoder.negative_air_value = -1\n",
    "    multilayer_stack_decoder.cutoff_point = 0.5\n",
    "    multilayer_stack_decoder.display_decoding = False\n",
    "    return multilayer_stack_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom environment\n",
    "class Environment:\n",
    "    def init(self, map, max_step=math.inf):\n",
    "        self.map = map\n",
    "        self.state = copy.deepcopy(map)\n",
    "        self.step_count = 0\n",
    "        self.max_step = max_step\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.deepcopy(self.map)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "\n",
    "        state_tensor = torch.tensor(self.state)\n",
    "        return state_tensor\n",
    "\n",
    "    def step(self, action, distance):\n",
    "        # add that pixel to the state (level map)\n",
    "        # action[0] is the x coordinate, action[1] is the y coordinate, action[2] is the type\n",
    "        self.state[action[0], action[1], action[2]] = 1\n",
    "        self.step_count += 1\n",
    "        # reward = self.reward_func(action[0], action[1], action[2], distance)\n",
    "        reward = 0\n",
    "\n",
    "        if self.step_count > self.max_step:\n",
    "            self.done = True\n",
    "\n",
    "        info = {'step': self.step_count, 'action': action, 'reward': reward, 'done': self.done}\n",
    "\n",
    "        return torch.tensor(self.state), torch.tensor(reward), self.done, info\n",
    "    \n",
    "    def reward_func(action, average_pixel):\n",
    "        # get the distance from average of the empty pixels\n",
    "        return 1/(1+math.sqrt((action[0] - average_pixel[0])**2 + (action[1] - average_pixel[1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"train_datasets/modified_test_run_200/modified_test_run_200.tfrecords\"\n",
    "modified_dataset = LevelDataset(dataset_path = dataset_path, batch_size = 1)\n",
    "modified_dataset.load_dataset()\n",
    "modified_iter_data = modified_dataset.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, data in modified_iter_data:\n",
    "    # print(data)\n",
    "    print(image_batch.shape)\n",
    "    modified = image_batch[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.GridSearchDecode import run_evaluation_xml_levels_one_by_one, create_tests\n",
    "\n",
    "parameters = create_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def move_file(source_file, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    if os.path.exists(source_file):\n",
    "        file_name = source_file.split(\"/\")[-1]\n",
    "        destination_file = os.path.join(destination_folder, file_name)\n",
    "        shutil.move(source_file, destination_file)\n",
    "        print(f\"File '{source_file}' moved successfully!\")\n",
    "        return destination_file\n",
    "    else:\n",
    "        print(f\"File '{source_file}' does not exist in the source folder.\")\n",
    "\n",
    "# Example usage:\n",
    "# move_file(\"path/to/source/folder\", \"path/to/destination/folder\", \"filename.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_distance(predicted_pixel, pixel_list, threshold=7):\n",
    "    min_distance = math.inf\n",
    "    for pixel in pixel_list:\n",
    "        distance = math.sqrt((predicted_pixel[0] - pixel[0])**2 + (predicted_pixel[1] - pixel[1])**2 + (predicted_pixel[2] - pixel[2])**2)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "\n",
    "    if min_distance < threshold:\n",
    "        return 1/(1+min_distance)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "pixel_list = [(0, 0, 0), (10, 10, 10), (20, 20, 20), (30, 30, 30)]\n",
    "predicted_pixel = (3, 3, 3)\n",
    "reward = find_closest_distance(predicted_pixel, pixel_list)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_step_calc(original, modified):\n",
    "    mask = tf.not_equal(original, modified)\n",
    "    indices = tf.where(mask)\n",
    "\n",
    "    num_differences = np.sum(mask)\n",
    "\n",
    "    return num_differences, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_differences, indices = max_step_calc(original, modified)\n",
    "# print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_removed_blocks(indices):\n",
    "    # create a 2d grid of values\n",
    "    grid = np.zeros((128, 128))\n",
    "    for index in indices:\n",
    "        grid[index[0], index[1]] += 1\n",
    "\n",
    "    # plot the grid\n",
    "    plt.imshow(grid, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(original, modified, predicted_pixel):\n",
    "    mask = tf.not_equal(original, modified)\n",
    "    indices = tf.where(mask)\n",
    "\n",
    "    # Print the positions where the elements are different\n",
    "    # print(\"Positions where elements are different:\")\n",
    "    # print(np.where(mask))\n",
    "\n",
    "    # Calculate the number of differences\n",
    "    num_differences = np.sum(mask)\n",
    "    print(\"Number of differences:\", num_differences)\n",
    "\n",
    "    pixel_list = []\n",
    "\n",
    "    for idx in indices:\n",
    "        print(idx)\n",
    "        # row_idx, col_idx, channel_idx = idx[0], idx[1], idx[2]\n",
    "        # original_value = tf.gather_nd(original, [idx])\n",
    "        # modified_value = tf.gather_nd(modified, [idx])\n",
    "        pixel_list.append(idx)\n",
    "        # print(f\"Difference at position {idx}: original={original_value.numpy()}, modified={modified_value.numpy()}\")\n",
    "\n",
    "    return find_closest_distance(predicted_pixel, pixel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from converter.gan_processing.DecodingFunctions import DecodingFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_convert(original, modified, path, counter, show_fig = False):\n",
    "    # functions to move the output from [-1, 1] to [0, 1] range\n",
    "    decoding_functions = DecodingFunctions(threshold_callback = lambda: 0.5)\n",
    "    decoding_functions.set_rescaling(rescaling = tf.keras.layers.Rescaling)\n",
    "    decoding_functions.update_rescale_values(max_value = 1, shift_value = 1)\n",
    "    rescale_function = decoding_functions.rescale\n",
    "\n",
    "    # function to flatten the gan output to an image with 1 channel\n",
    "    decoding_function = decoding_functions.argmax_multilayer_decoding_with_air\n",
    "\n",
    "    if show_fig:\n",
    "        ref_img, _ = decoding_function(original)\n",
    "        # print(gan_outputs_reformatted[i].shape)\n",
    "\n",
    "        # save image trough matplotlib\n",
    "        plt.imshow(ref_img)\n",
    "        plt.savefig(f'{path}/original_level{counter}.png')\n",
    "        # clear plot\n",
    "        plt.clf()\n",
    "\n",
    "        ref_img, _ = decoding_function(modified)\n",
    "        # print(gan_outputs_reformatted[i].shape)\n",
    "\n",
    "        # save image trough matplotlib\n",
    "        plt.imshow(ref_img)\n",
    "        plt.savefig(f'{path}/modified1_level{counter}.png')\n",
    "        # clear plot\n",
    "        plt.clf()\n",
    "\n",
    "    multilayer_stack_decoder = load_level_decoder()\n",
    "    # level_visualizer = LevelVisualizer()\n",
    "    level_reader = LevelReader()\n",
    "\n",
    "    \n",
    "    level = multilayer_stack_decoder.decode(modified)\n",
    "    # print(\"level\", level)\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, dpi = 100)\n",
    "    # level_visualizer.create_img_of_structure(\n",
    "    #     level.get_used_elements(), use_grid = False, ax = ax, scaled = True\n",
    "    # )\n",
    "    # fig.savefig('modified1_decoded_level{counter}.png')\n",
    "    # plt.clf()\n",
    "\n",
    "    # Save level to xml\n",
    "    level_xml = level_reader.create_level_from_structure(level.get_used_elements(), red_birds = True, move_to_ground = True)\n",
    "    level_reader.write_xml_file(level_xml, os.path.join(\"./\", f'{path}/modified_level{counter}.xml'))\n",
    "\n",
    "    return f'{path}/modified_level{counter}.xml'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a 2d heatmap\n",
    "# the heatmap is a 2d grid of values and a counter for repetition of each value\n",
    "def plot_heatmap(heatmap, title = \"default\"):\n",
    "    # create a 2d grid of values\n",
    "    grid = np.zeros((128, 128))\n",
    "    for key, value in heatmap.items():\n",
    "        grid[key[0], key[1]] = value\n",
    "\n",
    "    # plot the grid\n",
    "    plt.imshow(grid, cmap='hot', interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total(array, title = \"default\"):\n",
    "    loss_array = np.array(array)\n",
    "    plt.plot(loss_array)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Define a custom environment\n",
    "class Environment:\n",
    "    def __init__(self, map, max_step=1000):\n",
    "        self.map = map  # Initialize the map\n",
    "        self.state = copy.deepcopy(self.map)\n",
    "        self.step_count = 0\n",
    "        self.max_step = max_step\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.deepcopy(self.map)\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        state_tensor = torch.tensor(self.state, dtype=torch.float32).flatten()\n",
    "        return state_tensor\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y, z = action  # Decompose the action into coordinates\n",
    "        if self.state[x, y, z] == -1:\n",
    "            self.state[x, y, z] = 1  # Place block if the spot is empty\n",
    "            reward = 1  # Positive reward for placing a block\n",
    "        else:\n",
    "            reward = -10 # Negative reward if block is already there\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.max_step:\n",
    "            self.done = True\n",
    "        return torch.tensor(self.state, dtype=torch.float32).flatten(), reward, self.done, {}\n",
    "\n",
    "# PPO Model\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PPO, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_pi = nn.Linear(128, input_size)\n",
    "        self.fc_v = nn.Linear(128, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.03)\n",
    "        # self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.99)\n",
    "        self.data = []  # Initialize data list for storing transitions\n",
    "        self.total_loss = []\n",
    "        self.total_reward = []\n",
    "\n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = torch.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = zip(*self.data)\n",
    "        self.data = []\n",
    "        return torch.stack(s_lst), torch.tensor(a_lst, dtype=torch.long), torch.tensor(r_lst, dtype=torch.float), torch.stack(s_prime_lst), torch.tensor(done_lst, dtype=torch.float)\n",
    "\n",
    "    def train_net(self, gamma=0.98, lmbda=0.95, eps_clip=0.2):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "        td_target = r + gamma * self.v(s_prime) * (1 - done)\n",
    "        delta = td_target - self.v(s)\n",
    "        delta = delta.detach().numpy()\n",
    "        advantage_lst = []\n",
    "        advantage = 0.0\n",
    "        for delta_t in delta[::-1]:\n",
    "            advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "            advantage_lst.append([advantage])\n",
    "        advantage_lst.reverse()\n",
    "        advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        ratio = torch.exp(torch.log(pi_a) - torch.log(pi_a.detach()))\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "        entropy = -(pi * torch.log(pi + 1e-5)).sum(1).mean()\n",
    "        loss = -torch.min(surr1, surr2) + torch.nn.functional.smooth_l1_loss(self.v(s), td_target.detach()) - 0.01 * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_mean = loss.mean()  # Compute mean loss\n",
    "        loss_mean.backward()\n",
    "        self.optimizer.step()\n",
    "        # self.scheduler.step()\n",
    "        return loss_mean  # Return the mean loss value\n",
    "    \n",
    "    \n",
    "def main(iter_data, modified_iter_data):\n",
    "    model = PPO(128*128*5)  # Initialize the PPO model\n",
    "    score = 0.0\n",
    "    counter = 0\n",
    "    combined_dataset = tf.data.Dataset.zip((iter_data, modified_iter_data))\n",
    "    distance_threshold = 40\n",
    "    learn_threshold = 1\n",
    "\n",
    "    for (image_batch, data), (modified_image_batch, modified_data) in combined_dataset:\n",
    "        \n",
    "        original = image_batch[0].numpy()\n",
    "        closest_score = 0.0\n",
    "        score = 0.0\n",
    "        env = Environment(modified_image_batch[0].numpy())  # Create environment from batch\n",
    "        state = env.reset()  # Reset environment at the start of each batch\n",
    "        diff, indices  = max_step_calc(original, env.state)\n",
    "        if counter % 20 == 0:\n",
    "            plot_removed_blocks(indices)\n",
    "        if 0 == diff or diff > 400:\n",
    "            continue\n",
    "        env.max_step = diff\n",
    "        heatmap_dict = {}\n",
    "        empty_point_dict = {}\n",
    "        done = False\n",
    "        while not done:\n",
    "            prob = model.pi(state)  # Policy forward pass\n",
    "            m = Categorical(prob)  # Distribution for sampling actions\n",
    "            action_index = m.sample().item()  # Sample an action\n",
    "            action = action_index // (128*5), (action_index % (128*5)) // 5, action_index % 5\n",
    "            heatmap_dict[(action[0], action[1])] = heatmap_dict.get((action[0], action[1]), 0) + 1\n",
    "            state_prime, reward, done, _ = env.step(action)  # Execute action in the environment\n",
    "            # normalize the reward\n",
    "            reward = reward / env.max_step\n",
    "            state = state_prime\n",
    "            score += reward  # Update score\n",
    "            if reward < 0:\n",
    "                empty_point_dict[(action[0], action[1])] =  - 1\n",
    "            else:\n",
    "                empty_point_dict[(action[0], action[1])] = 1\n",
    "            closest= find_closest_distance(action, pixel_list, distance_threshold)\n",
    "            closest = closest / env.max_step\n",
    "            closest_score += closest\n",
    "            score += closest\n",
    "\n",
    "            model.put_data((state, action_index, reward + closest, state_prime, done))  # Store data for training\n",
    "            if counter % learn_threshold == 0:\n",
    "                loss = model.train_net()  # Train model\n",
    "        \n",
    "        xml_path = xml_convert(original, env.state, \"temp\", counter, show_fig = True)\n",
    "        final_path = move_file(xml_path, 'evaluation\\\\temp')\n",
    "        # # if level is stable\n",
    "        # if run_evaluation_xml_levels_one_by_one(\"temp\", parameters[0]):\n",
    "        #     print(\"Level is stable\")\n",
    "        #     score += 10\n",
    "        # else:\n",
    "        #     print(\"Level is not stable\")\n",
    "        #     score -= 10\n",
    "        # os.remove(final_path)\n",
    "\n",
    "        print(f\"distance score: {closest_score}\")\n",
    "        print(f\"Episode Score: {score}, Loss: {loss}\")\n",
    "        print(f\"Episode {counter} completed\")\n",
    "        model.total_loss.append(loss)\n",
    "        model.total_reward.append(score)\n",
    "        print(\"##############################################\")\n",
    "        counter += 1\n",
    "        if counter % 20 == 0:\n",
    "            plot_heatmap(empty_point_dict, title = \"empty points\")\n",
    "            plot_heatmap(heatmap_dict, title = \"total points\")\n",
    "            print(f\"toall difference: {diff}\")\n",
    "            diff, _ = max_step_calc(original, env.state)\n",
    "            print(f\"toall difference after modifying: {diff}\")\n",
    "            wrong_filled = empty_point_dict.values()\n",
    "            wrong_filled = list(filter(lambda x: x < 0, wrong_filled))\n",
    "            print(f\"number of wrong filled pixels: {len(wrong_filled)}\")\n",
    "\n",
    "            print(f\"Episode {counter} completed\")\n",
    "            plot_total(model.total_reward, title = \"total reward\")\n",
    "            print(\"###############################################################################\")\n",
    "            if distance_threshold > 4:\n",
    "                distance_threshold -= 2\n",
    "            if learn_threshold < 100:\n",
    "                learn_threshold += 1\n",
    "            # Save the model\n",
    "            print(\"Model saved successfully\")\n",
    "            torch.save(model.state_dict(), f'temp_models/ppo_model{counter}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions.categorical import Categorical\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions.categorical import Categorical\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Categorical\n",
    "# import numpy as np\n",
    "# import copy\n",
    "\n",
    "# # Define your environment class here (unchanged)\n",
    "# class BlockPlacingEnvironment:\n",
    "#     def __init__(self, map, max_step=500):\n",
    "#         self.map = map\n",
    "#         self.state = copy.deepcopy(self.map)\n",
    "#         self.step_count = 0\n",
    "#         self.max_step = max_step\n",
    "#         self.done = False\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.state = copy.deepcopy(self.map)\n",
    "#         self.step_count = 0\n",
    "#         self.done = False\n",
    "#         state_tensor = torch.tensor(self.state, dtype=torch.float32).flatten()\n",
    "#         return state_tensor\n",
    "\n",
    "#     def step(self, action):\n",
    "#         # print(action)\n",
    "#         z = action % 5\n",
    "#         x = action // (128*5)\n",
    "#         y = (action % (128*5)) // 5\n",
    "#         if self.state[x, y, z] == -1:\n",
    "#             self.state[x, y, z] = 1\n",
    "#             reward = 10\n",
    "#         else:\n",
    "#             reward = -10\n",
    "#         self.step_count += 1\n",
    "#         if self.step_count >= self.max_step:\n",
    "#             self.done = True\n",
    "#         return torch.tensor(self.state, dtype=torch.float32).flatten(), reward, self.done\n",
    "\n",
    "# class PPOMemory:\n",
    "#     def __init__(self, batch_size):\n",
    "#         self.states = []\n",
    "#         self.probs = []\n",
    "#         self.vals = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.dones = []\n",
    "\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def generate_batches(self):\n",
    "#         n_states = len(self.states)\n",
    "#         batch_start = np.arange(0, n_states, self.batch_size)\n",
    "#         indices = np.arange(n_states, dtype=np.int64)\n",
    "#         np.random.shuffle(indices)\n",
    "#         batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "#         return np.array(self.states),\\\n",
    "#                 np.array(self.actions),\\\n",
    "#                 np.array(self.probs),\\\n",
    "#                 np.array(self.vals),\\\n",
    "#                 np.array(self.rewards),\\\n",
    "#                 np.array(self.dones),\\\n",
    "#                 batches\n",
    "\n",
    "#     def store_memory(self, state, action, probs, vals, reward, done):\n",
    "#         self.states.append(state)\n",
    "#         self.actions.append(action)\n",
    "#         self.probs.append(probs)\n",
    "#         self.vals.append(vals)\n",
    "#         self.rewards.append(reward)\n",
    "#         self.dones.append(done)\n",
    "\n",
    "#     def clear_memory(self):\n",
    "#         self.states = []\n",
    "#         self.probs = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.dones = []\n",
    "#         self.vals = []\n",
    "\n",
    "# class ActorNetwork(nn.Module):\n",
    "#     def __init__(self, n_actions, input_dims, alpha,\n",
    "#             fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo'):\n",
    "#         super(ActorNetwork, self).__init__()\n",
    "\n",
    "#         self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "#         self.actor = nn.Sequential(\n",
    "#                 nn.Linear(*input_dims, fc1_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc1_dims, fc2_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc2_dims, n_actions),\n",
    "#                 nn.Softmax(dim=-1)\n",
    "#         )\n",
    "\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "#         self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "#         self.to(self.device)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         dist = self.actor(state)\n",
    "#         dist = Categorical(dist)\n",
    "        \n",
    "#         return dist\n",
    "\n",
    "#     def save_checkpoint(self):\n",
    "#         T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "#     def load_checkpoint(self):\n",
    "#         self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "# class CriticNetwork(nn.Module):\n",
    "#     def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "#             chkpt_dir='tmp/ppo'):\n",
    "#         super(CriticNetwork, self).__init__()\n",
    "\n",
    "#         self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "#         self.critic = nn.Sequential(\n",
    "#                 nn.Linear(*input_dims, fc1_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc1_dims, fc2_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc2_dims, 1)\n",
    "#         )\n",
    "\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "#         self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "#         self.to(self.device)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         value = self.critic(state)\n",
    "\n",
    "#         return value\n",
    "\n",
    "#     def save_checkpoint(self):\n",
    "#         T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "#     def load_checkpoint(self):\n",
    "#         self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "#             policy_clip=0.2, batch_size=64, n_epochs=10):\n",
    "#         self.gamma = gamma\n",
    "#         self.policy_clip = policy_clip\n",
    "#         self.n_epochs = n_epochs\n",
    "#         self.gae_lambda = gae_lambda\n",
    "\n",
    "#         self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "#         self.critic = CriticNetwork(input_dims, alpha)\n",
    "#         self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "#     def remember(self, state, action, probs, vals, reward, done):\n",
    "#         self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "#     def save_models(self):\n",
    "#         print('... saving models ...')\n",
    "#         self.actor.save_checkpoint()\n",
    "#         self.critic.save_checkpoint()\n",
    "\n",
    "#     def load_models(self):\n",
    "#         print('... loading models ...')\n",
    "#         self.actor.load_checkpoint()\n",
    "#         self.critic.load_checkpoint()\n",
    "\n",
    "#     def choose_action(self, observation):\n",
    "#         state = observation\n",
    "\n",
    "#         dist = self.actor(state)\n",
    "#         value = self.critic(state)\n",
    "#         action = dist.sample()\n",
    "\n",
    "#         probs = T.squeeze(dist.log_prob(action)).item()\n",
    "#         action = T.squeeze(action).item()\n",
    "#         value = T.squeeze(value).item()\n",
    "\n",
    "#         return action, probs, value\n",
    "\n",
    "\n",
    "#     def learn(self):\n",
    "#         for _ in range(self.n_epochs):\n",
    "#             state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "#             reward_arr, dones_arr, batches = \\\n",
    "#                     self.memory.generate_batches()\n",
    "\n",
    "#             values = vals_arr\n",
    "#             advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "#             for t in range(len(reward_arr)-1):\n",
    "#                 discount = 1\n",
    "#                 a_t = 0\n",
    "#                 for k in range(t, len(reward_arr)-1):\n",
    "#                     a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "#                             (1-int(dones_arr[k])) - values[k])\n",
    "#                     discount *= self.gamma*self.gae_lambda\n",
    "#                 advantage[t] = a_t\n",
    "#             advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "#             values = T.tensor(values).to(self.actor.device)\n",
    "#             for batch in batches:\n",
    "#                 print(f\"state arr: {state_arr[batch]}\")\n",
    "#                 states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "#                 old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "#                 actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "#                 dist = self.actor(states)\n",
    "#                 critic_value = self.critic(states)\n",
    "\n",
    "#                 critic_value = T.squeeze(critic_value)\n",
    "\n",
    "#                 new_probs = dist.log_prob(actions)\n",
    "#                 prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "#                 #prob_ratio = (new_probs - old_probs).exp()\n",
    "#                 weighted_probs = advantage[batch] * prob_ratio\n",
    "#                 weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "#                         1+self.policy_clip)*advantage[batch]\n",
    "#                 actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "#                 returns = advantage[batch] + values[batch]\n",
    "#                 critic_loss = (returns-critic_value)**2\n",
    "#                 critic_loss = critic_loss.mean()\n",
    "\n",
    "#                 total_loss = actor_loss + 0.5*critic_loss\n",
    "#                 self.actor.optimizer.zero_grad()\n",
    "#                 self.critic.optimizer.zero_grad()\n",
    "#                 total_loss.backward()\n",
    "#                 self.actor.optimizer.step()\n",
    "#                 self.critic.optimizer.step()\n",
    "\n",
    "#         self.memory.clear_memory()    \n",
    "\n",
    "#         print(\"Average Actor Loss:\", actor_loss.item())\n",
    "#         print(\"Average Critic Loss:\", critic_loss.item())\n",
    "#         print(\"Actor Weights Norm:\", sum(p.norm().item() for p in self.actor.parameters()))\n",
    "#         print(\"Critic Weights Norm:\", sum(p.norm().item() for p in self.critic.parameters()))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     agent = Agent(n_actions=128*128*5, input_dims=(128*128*5,), alpha=0.0001, batch_size=64, n_epochs=5)\n",
    "#     episode = 0\n",
    "#     for image_batch, data in modified_iter_data:\n",
    "#         env = BlockPlacingEnvironment(image_batch[0].numpy())\n",
    "#         observation = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             action, probs, value = agent.choose_action(observation)\n",
    "#             observation, reward, done = env.step(action)\n",
    "#             agent.remember(observation, action, probs, value, reward, done)\n",
    "#             total_reward += reward\n",
    "\n",
    "#         agent.learn()\n",
    "#         print(f'Episode {episode + 1}: Total Reward = {total_reward}')\n",
    "#         episode += 1\n",
    "#     agent.save_models()           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions.categorical import Categorical\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch as T\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions.categorical import Categorical\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Categorical\n",
    "# import numpy as np\n",
    "# import copy\n",
    "\n",
    "# # Define your environment class here (unchanged)\n",
    "# class BlockPlacingEnvironment:\n",
    "#     def __init__(self, map, max_step=500):\n",
    "#         self.map = map\n",
    "#         self.state = copy.deepcopy(self.map)\n",
    "#         self.step_count = 0\n",
    "#         self.max_step = max_step\n",
    "#         self.done = False\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.state = copy.deepcopy(self.map)\n",
    "#         self.step_count = 0\n",
    "#         self.done = False\n",
    "#         state_tensor = torch.tensor(self.state, dtype=torch.float32).flatten()\n",
    "#         return state_tensor\n",
    "\n",
    "#     def step(self, action):\n",
    "#         # print(action)\n",
    "#         z = action % 5\n",
    "#         x = action // (128*5)\n",
    "#         y = (action % (128*5)) // 5\n",
    "#         if self.state[x, y, z] == -1:\n",
    "#             self.state[x, y, z] = 1\n",
    "#             reward = 10\n",
    "#         else:\n",
    "#             reward = -10\n",
    "#         self.step_count += 1\n",
    "#         if self.step_count >= self.max_step:\n",
    "#             self.done = True\n",
    "#         return torch.tensor(self.state, dtype=torch.float32).flatten(), reward, self.done\n",
    "\n",
    "# class PPOMemory:\n",
    "#     def __init__(self, batch_size):\n",
    "#         self.states = []\n",
    "#         self.probs = []\n",
    "#         self.vals = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.dones = []\n",
    "\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def generate_batches(self):\n",
    "#         n_states = len(self.states)\n",
    "#         batch_start = np.arange(0, n_states, self.batch_size)\n",
    "#         indices = np.arange(n_states, dtype=np.int64)\n",
    "#         np.random.shuffle(indices)\n",
    "#         batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "#         return np.array(self.states),\\\n",
    "#                 np.array(self.actions),\\\n",
    "#                 np.array(self.probs),\\\n",
    "#                 np.array(self.vals),\\\n",
    "#                 np.array(self.rewards),\\\n",
    "#                 np.array(self.dones),\\\n",
    "#                 batches\n",
    "\n",
    "#     def store_memory(self, state, action, probs, vals, reward, done):\n",
    "#         self.states.append(state)\n",
    "#         self.actions.append(action)\n",
    "#         self.probs.append(probs)\n",
    "#         self.vals.append(vals)\n",
    "#         self.rewards.append(reward)\n",
    "#         self.dones.append(done)\n",
    "\n",
    "#     def clear_memory(self):\n",
    "#         self.states = []\n",
    "#         self.probs = []\n",
    "#         self.actions = []\n",
    "#         self.rewards = []\n",
    "#         self.dones = []\n",
    "#         self.vals = []\n",
    "\n",
    "# class ActorNetwork(nn.Module):\n",
    "#     def __init__(self, n_actions, input_dims, alpha,\n",
    "#             fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo'):\n",
    "#         super(ActorNetwork, self).__init__()\n",
    "\n",
    "#         self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "#         self.actor = nn.Sequential(\n",
    "#                 nn.Linear(*input_dims, fc1_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc1_dims, fc2_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc2_dims, n_actions),\n",
    "#                 nn.Softmax(dim=-1)\n",
    "#         )\n",
    "\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "#         self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "#         self.to(self.device)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         dist = self.actor(state)\n",
    "#         dist = Categorical(dist)\n",
    "        \n",
    "#         return dist\n",
    "\n",
    "#     def save_checkpoint(self):\n",
    "#         T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "#     def load_checkpoint(self):\n",
    "#         self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "# class CriticNetwork(nn.Module):\n",
    "#     def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "#             chkpt_dir='tmp/ppo'):\n",
    "#         super(CriticNetwork, self).__init__()\n",
    "\n",
    "#         self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "#         self.critic = nn.Sequential(\n",
    "#                 nn.Linear(*input_dims, fc1_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc1_dims, fc2_dims),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(fc2_dims, 1)\n",
    "#         )\n",
    "\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "#         self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "#         self.to(self.device)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         value = self.critic(state)\n",
    "\n",
    "#         return value\n",
    "\n",
    "#     def save_checkpoint(self):\n",
    "#         T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "#     def load_checkpoint(self):\n",
    "#         self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "#             policy_clip=0.2, batch_size=64, n_epochs=10):\n",
    "#         self.gamma = gamma\n",
    "#         self.policy_clip = policy_clip\n",
    "#         self.n_epochs = n_epochs\n",
    "#         self.gae_lambda = gae_lambda\n",
    "\n",
    "#         self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "#         self.critic = CriticNetwork(input_dims, alpha)\n",
    "#         self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "#     def remember(self, state, action, probs, vals, reward, done):\n",
    "#         self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "#     def save_models(self):\n",
    "#         print('... saving models ...')\n",
    "#         self.actor.save_checkpoint()\n",
    "#         self.critic.save_checkpoint()\n",
    "\n",
    "#     def load_models(self):\n",
    "#         print('... loading models ...')\n",
    "#         self.actor.load_checkpoint()\n",
    "#         self.critic.load_checkpoint()\n",
    "\n",
    "#     def choose_action(self, observation):\n",
    "#         state = observation\n",
    "\n",
    "#         dist = self.actor(state)\n",
    "#         value = self.critic(state)\n",
    "#         action = dist.sample()\n",
    "\n",
    "#         probs = T.squeeze(dist.log_prob(action)).item()\n",
    "#         action = T.squeeze(action).item()\n",
    "#         value = T.squeeze(value).item()\n",
    "\n",
    "#         return action, probs, value\n",
    "\n",
    "\n",
    "#     def learn(self):\n",
    "#         for _ in range(self.n_epochs):\n",
    "#             state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "#             reward_arr, dones_arr, batches = \\\n",
    "#                     self.memory.generate_batches()\n",
    "\n",
    "#             values = vals_arr\n",
    "#             advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "#             for t in range(len(reward_arr)-1):\n",
    "#                 discount = 1\n",
    "#                 a_t = 0\n",
    "#                 for k in range(t, len(reward_arr)-1):\n",
    "#                     a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "#                             (1-int(dones_arr[k])) - values[k])\n",
    "#                     discount *= self.gamma*self.gae_lambda\n",
    "#                 advantage[t] = a_t\n",
    "#             advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "#             values = T.tensor(values).to(self.actor.device)\n",
    "#             for batch in batches:\n",
    "#                 states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "#                 old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "#                 actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "#                 dist = self.actor(states)\n",
    "#                 critic_value = self.critic(states)\n",
    "\n",
    "#                 critic_value = T.squeeze(critic_value)\n",
    "\n",
    "#                 new_probs = dist.log_prob(actions)\n",
    "#                 prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "#                 #prob_ratio = (new_probs - old_probs).exp()\n",
    "#                 weighted_probs = advantage[batch] * prob_ratio\n",
    "#                 weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "#                         1+self.policy_clip)*advantage[batch]\n",
    "#                 actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "#                 returns = advantage[batch] + values[batch]\n",
    "#                 critic_loss = (returns-critic_value)**2\n",
    "#                 critic_loss = critic_loss.mean()\n",
    "\n",
    "#                 total_loss = actor_loss + 0.5*critic_loss\n",
    "#                 self.actor.optimizer.zero_grad()\n",
    "#                 self.critic.optimizer.zero_grad()\n",
    "#                 total_loss.backward()\n",
    "#                 self.actor.optimizer.step()\n",
    "#                 self.critic.optimizer.step()\n",
    "\n",
    "#         self.memory.clear_memory()    \n",
    "\n",
    "#         print(\"Average Actor Loss:\", actor_loss.item())\n",
    "#         print(\"Average Critic Loss:\", critic_loss.item())\n",
    "#         print(\"Actor Weights Norm:\", sum(p.norm().item() for p in self.actor.parameters()))\n",
    "#         print(\"Critic Weights Norm:\", sum(p.norm().item() for p in self.critic.parameters()))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     agent = Agent(n_actions=128*128*5, input_dims=(128*128*5,), alpha=0.0001, batch_size=64, n_epochs=5)\n",
    "#     episode = 0\n",
    "#     for image_batch, data in modified_iter_data:\n",
    "#         env = BlockPlacingEnvironment(image_batch[0].numpy())\n",
    "#         observation = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "\n",
    "#         diff, indices  = max_step_calc(original, env.state)\n",
    "#         plot_removed_blocks(indices)\n",
    "#         env.max_step = diff\n",
    "#         heatmap_dict = {}\n",
    "#         empty_point_dict = {}\n",
    "#         closest_score = 0\n",
    "#         distance_threshold = 40\n",
    "\n",
    "\n",
    "#         while not done:\n",
    "#             action, probs, value = agent.choose_action(observation)\n",
    "#             observation, reward, done = env.step(action)\n",
    "#             x = action // (128*5)\n",
    "#             y = (action % (128*5)) // 5\n",
    "#             z = action % 5\n",
    "#             print(f\"Action: {x}, {y}, {z}\")\n",
    "\n",
    "#             heatmap_dict[(x, y)] = heatmap_dict.get((x, y), 0) + 1\n",
    "#             state_prime, reward, done = env.step(action)  # Execute action in the environment\n",
    "#             # normalize the reward\n",
    "#             reward = reward / env.max_step\n",
    "#             state = state_prime\n",
    "#             total_reward += reward  # Update score\n",
    "#             if reward < 0:\n",
    "#                 empty_point_dict[(x,y)] =  - 1\n",
    "#             else:\n",
    "#                 empty_point_dict[(x,y)] = 1\n",
    "#             closest= find_closest_distance((x,y,z), pixel_list, distance_threshold)\n",
    "#             closest = closest / env.max_step\n",
    "#             closest_score += closest\n",
    "#             total_reward += closest\n",
    "\n",
    "#             agent.remember(observation, action, probs, value, reward, done)\n",
    "            \n",
    "#         agent.learn()\n",
    "#         plot_heatmap(empty_point_dict, title = \"empty points\")\n",
    "#         plot_heatmap(heatmap_dict, title = \"total points\")\n",
    "#         print(f\"toall difference: {diff}\")\n",
    "#         diff, _ = max_step_calc(original, env.state)\n",
    "#         print(f\"toall difference after modifying: {diff}\")\n",
    "#         wrong_filled = empty_point_dict.values()\n",
    "#         wrong_filled = list(filter(lambda x: x < 0, wrong_filled))\n",
    "#         print(f\"number of wrong filled pixels: {len(wrong_filled)}\")\n",
    "#         xml_path = xml_convert(original, env.state, \"temp\", episode, show_fig = True)\n",
    "#         final_path = move_file(xml_path, 'evaluation\\\\temp')\n",
    "#         # # if level is stable\n",
    "#         # if run_evaluation_xml_levels_one_by_one(\"temp\", parameters[0]):\n",
    "#         #     print(\"Level is stable\")\n",
    "#         #     score += 10\n",
    "#         # else:\n",
    "#         #     print(\"Level is not stable\")\n",
    "#         #     score -= 10\n",
    "#         # os.remove(final_path)\n",
    "#         print(f\"Episode Score: {total_reward}, Closest Score: {closest_score}\")\n",
    "#         print(\"##############################################\")\n",
    "#         episode += 1\n",
    "#         if episode % 20 == 0:\n",
    "#             print(f\"Episode {episode} completed\")\n",
    "            \n",
    "#             if distance_threshold > 4:\n",
    "#                 distance_threshold -= 2\n",
    "                \n",
    "#         episode += 1\n",
    "#     agent.save_models()           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Assume modified_iter_data is your dataset\n",
    "    main(iter_data, modified_iter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "print(\"Loading ...\")\n",
    "model = PPO(128*128*5)  # Recreate the model\n",
    "model.load_state_dict(torch.load('ppo_model.pth'))  # Load the saved parameters\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for image_batch, data in modified_iter_data:  # Assuming test data is provided in test_iter_data\n",
    "        state = env.reset()  # Reset the environment\n",
    "        done = False\n",
    "        while not done:\n",
    "            prob = model.pi(state)\n",
    "            m = Categorical(prob)\n",
    "            action_index = m.sample().item()\n",
    "            action = (action_index // (128*128), (action_index % (128*128)) // 128, action_index % 5)  \n",
    "            state_prime, reward, done, _ = env.step(action)\n",
    "            state = state_prime\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for idx in indices:\n",
    "#     count += 1\n",
    "\n",
    "#     row_idx, col_idx, channel_idx = idx[0], idx[1], idx[2]\n",
    "#     original_value = tf.gather_nd(original, [idx])  # Get original value\n",
    "#     modified = tf.tensor_scatter_nd_update(\n",
    "#         modified, [[row_idx, col_idx, channel_idx]], original_value  # Update modified tensor\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Modified tensor with original values rewritten:\")\n",
    "# print(modified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
